
# About #

Personal "iterative" kubernetes sandbox used while playing with kubernetes at home on top of raspberry pi 3B machines.

We'll eventually use ansible to automate the initial setup.

We'll first setup a single-master kubernetes cluster on top of 4x Raspberry pi 3B (our fifth raspberry pi ethernet device died recently).

Because one raspberry pi 3B can barely handle the load generated by 4 worker nodes calls to apiserver (no comment), we'll soon switch to a multi-master cluster, composed of :
- 3 master nodes
- 1 worker node (later on we'll add amd64 nodes to the cluster, entering the painful world of docker and multi-arch images :p)
- 2x HAproxy + VRRP

references :
- https://github.com/alexellis/k8s-on-raspbian/blob/master/GUIDE.md


# Hosts #

In the first iteration/step, we'll use following raspberry pi hosts :

- pi01.p13.p.s18m2.com 10.13.1.21
- pi02.p13.p.s18m2.com 10.13.1.22
- pi03.p13.p.s18m2.com 10.13.1.23
- pi04.p13.p.s18m2.com 10.13.1.24 (master node)

One single kubernetes master node for now.


# hosts preparation #

Note : i won't add this ansible part to the repository, for multiple reasons (those initial playbooks and roles are quite ugly and still contain many hardcoded values, plus ansible will probably get dozens of deprecation warnings then errors in a few weeks, as usual...)

## configure local nameservers and networking ##

We setup two local nameservers to provide host name resolution for our sandbox zone "p13.p.s18m2.com" using :

```
  ansible-playbook -i inventory/bootstrap/ _bootstrap.yml
```

## install docker-ce ##

```
  cd ansible/
  ansible-playbook -i inventory/sandbox/ book_docker_engine.yml
```

## disable swap ##

```
  dphys-swapfile swapoff
  dphys-swapfile uninstall
  update-rc.d dphys-swapfile remove
```

## cgroups ##

```
  # append to /boot/cmdline.txt :
  cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory swapaccount=1
```

# Kubernetes setup using kubeadm #

## install kubeadm ##

```
  curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
  # yes, this shows "xenial" although we're running raspbian on our raspberry pi machines:
  echo "deb http://apt.kubernetes.io/ kubernetes-xenial main" > /etc/apt/sources.list.d/kubernetes.list
  apt-get update && apt-get -y install kubeadm
```

## kubernetes root dir ##

we'll use a separate partition (hopefully faster than internal mmc card) for the whole set of kubernetes tools.

we'll use /opt/hosting/infra/k8s ; /opt/hosting/infra being already mounted locally

```
  # on al nodes :
  mkdir /opt/hosting/infra/k8s
  sed -i 's#KUBELET_EXTRA_ARGS=#KUBELET_EXTRA_ARGS="--root-dir=/opt/hosting/infra/k8s"#g' /etc/default/kubelet
```



# master node init #

on future master node (pi04.p13.p.s18m2.com in our case), as root :

```
  # pre-pull required docker images to run a k8s master node :
  kubeadm config images pull

```

We will use 'flannel' networking, so we have to run (10.244.0.0/16 being the default flannel cidr) :

```
  # as root, on pi04 (future master node) :
  # 10.13.1.24 = pi04, our future master node (where we run those commands) :
  kubeadm init --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=10.13.1.24
```

or better : use this script in the first place to kubeadm init the cluster :
(chicken and egg problem on slower machines : kubeadm init creates the manifests containing .. the timeouts values we need to increase..)


```
#!/usr/bin/env python
#
# replace some startup timeouts when manifests files are created by kubeadm init
# see https://github.com/kubernetes/kubeadm/issues/413
# run me using 'sudo python foobar.py'
#
import os
import time
import threading

filepath = '/etc/kubernetes/manifests/kube-apiserver.yaml'

def replace_defaults():
    print('Thread start looking for the file')
    while not os.path.isfile(filepath):
        time.sleep(1) #wait one second
    print('\033[94m -----------> FILE FOUND: replacing defaults \033[0m')
    os.system("""sed -i 's/failureThreshold: [0-9]/failureThreshold: 18/g' /etc/kubernetes/manifests/kube-apiserver.yaml""")
    os.system("""sed -i 's/timeoutSeconds: [0-9][0-9]/timeoutSeconds: 20/g' /etc/kubernetes/manifests/kube-apiserver.yaml""")
    os.system("""sed -i 's/initialDelaySeconds: [0-9][0-9]/initialDelaySeconds: 240/g' /etc/kubernetes/manifests/kube-apiserver.yaml""")

t = threading.Thread(target=replace_defaults)
t.start()
os.system("kubeadm init --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=10.13.1.24")
```

as a regular user on same node (future master node, pi04) (as told by kubeadm init) :

```
  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

write join command somewhere, here's a sample :

```
kubeadm join 10.13.1.24:6443 --token ymduph.d5tuo85q088e1k72 --discovery-token-ca-cert-hash sha256:e568747339f7deaf29b6777d9bffd52766bf5fee22e0b94394bf4f49b9dcdb03
```


on master node (pi04), as the regular user with .kube/config :

```
  kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
  kubectl get pods --namespace=kube-system
```


on each node, including master node :

```
  sudo sysctl net.bridge.bridge-nf-call-iptables=1
```

as root, on every other node :

```
  # run "kubeadm token create --print-join-command" as root on master node to get a new token if lost :
  kubeadm join 10.13.1.24:6443 --token ymduph.d5tuo85q088e1k72 --discovery-token-ca-cert-hash sha256:e568747339f7deaf29b6777d9bffd52766bf5fee22e0b94394bf4f49b9dcdb03
```

# next - basic application + service with LoadBalancer type on baremetal (arm) #

exposing the service onto an external ip address :

To be able to use the spec.type: LoadBalancer, we install an ingress controller (nginx in our case).

We also setup metallb, so we get dynamically allocated 'external' ip addresses from configured pool.

https://kubernetes.github.io/ingress-nginx/deploy/


## nginx ingress controller ##

Note that we had to use another image, because of our ARM arch, hence the use of ./ingress/nginx/mandatory.yaml

```
# amd64 only: kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml
# arm:
kubectl apply -f ./ingress/nginx/mandatory.yaml
# to see progress :
kubectl describe pods -n ingress-nginx
```

then
```
#kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/provider/baremetal/service-nodeport.yaml
kubectl apply -f ./ingress/nginx/service-nodeport.yaml
```

## MetalLB ##

Finally, MetalLB controller needs to be installed, so our exposed services (with type LoadBalancer) gets external IP addresses:

```
  # https://metallb.universe.tf/installation/
  kubectl apply -f https://raw.githubusercontent.com/google/metallb/v0.7.3/manifests/metallb.yaml
  # MetalLBâ€™s components will still start, but will remain idle until you define and deploy a configmap :
  kubectl apply -f ./ingress/metallb/layer2-config.yaml

```

## test service ##

nginx hello world with spec.type: LoadBalancer (dynamically getting an external ip address + Loadbalancer)

1. create nginx deployment :

this creates a nginx pod, only accessible from within the cluster.
(using 'nginx:stable' image, working properly on arm arch)

```
  kubectl create -f ./deployments/helloworld-nginx.yml
  kubectl get pods -o wide
```

2. exposing service to whole cluster :

we create a service, exposing the nginx application within the entire cluster.

either :

```
  kubectl expose deployment/my-nginx
```

or

```
  kubectl create -f services/helloworld-svc-nginx.yml
```

```
  kubectl get svc -o wide my-nginx
  kubectl describe svc my-nginx

  kubectl scale --current-replicas=2 --replicas=1 deployment/my-nginx
```


See if nginx ingress controller and metallb did what they were supposed to :

```
$ kubectl get svc -o wide
NAME         TYPE           CLUSTER-IP      EXTERNAL-IP       PORT(S)        AGE    SELECTOR
kubernetes   ClusterIP      10.96.0.1       <none>            443/TCP        6h7m   <none>
my-nginx     LoadBalancer   10.101.142.20   192.168.100.240   80:30020/TCP   16m    run=my-nginx
```

We got an 'EXTERNAL-IP' assigned, in the range configured within metallb config :p

```
me@machine_external_to_cluster$ curl -v http://192.168.100.240/
*   Trying 192.168.100.240...
* TCP_NODELAY set
* Connected to 192.168.100.240 (192.168.100.240) port 80 (#0)
> GET / HTTP/1.1
> Host: 192.168.100.240
> User-Agent: curl/7.62.0
> Accept: */*
> 
< HTTP/1.1 200 OK
< Server: nginx/1.14.2
< Date: Thu, 31 Jan 2019 10:56:08 GMT
< Content-Type: text/html
< Content-Length: 612
< Last-Modified: Tue, 04 Dec 2018 14:44:49 GMT
< Connection: keep-alive
< ETag: "5c0692e1-264"
< Accept-Ranges: bytes
< 
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
...
```



# documentations / links #

- https://kubernetes.io/docs/tutorials/


# dashboard #

@TODO try kubernetes dashboard

ref : https://github.com/kubernetes/dashboard/wiki/Access-control#admin-privileges


# allow pods on master nodes #

optional : if you want to allow kubernetes master node to run tasks

```
  # remove 'master' role from master nodes, so they also accept tasks:
  kubectl taint nodes --all node-role.kubernetes.io/master-
```


# kubernetes commands #

```
  kubectl get pods --all-namespaces

  kubectl get pods --namespace=kube-system

  kubectl get nodes

  kubectl logs --tail=50 kube-flannel-ds-arm-rdk7f -n kube-system

  # list services in kube-system namespace :
  kubectl get svc -n kube-system

  kubectl apply -f <file.yml | url_to_yml>
  kubectl delete -f <file.yml | url_to_yml>

  # basic health report :
  kubectl get cs

  # basic :
  kubectl run appname --image=myimage:tag

  # useful when in 'creating' status:
  kubectl describe pods -n ingress-nginx

```



# teardown #

```
kubectl drain pi03.p13.p.s18m2.com --delete-local-data --force --ignore-daemonsets ; kubectl delete node pi03.p13.p.s18m2.com
kubectl drain pi04.p13.p.s18m2.com --delete-local-data --force --ignore-daemonsets ; kubectl delete node pi04.p13.p.s18m2.com
kubectl drain pi01.p13.p.s18m2.com --delete-local-data --force --ignore-daemonsets ; kubectl delete node pi01.p13.p.s18m2.com
kubectl drain pi02.p13.p.s18m2.com --delete-local-data --force --ignore-daemonsets ; kubectl delete node pi02.p13.p.s18m2.com
iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X
ipvsadm -C

```

https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#tear-down

Tear down

To undo what kubeadm did, you should first drain the node and make sure that the node is empty before shutting it down.

Talking to the master with the appropriate credentials, run:

kubectl drain <node name> --delete-local-data --force --ignore-daemonsets
kubectl delete node <node name>

Then, on the node being removed, reset all kubeadm installed state:

kubeadm reset

The reset process does not reset or clean up iptables rules or IPVS tables. If you wish to reset iptables, you must do so manually:

iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X

If you want to reset the IPVS tables, you must run the following command:

ipvsadm -C

If you wish to start over simply run kubeadm init or kubeadm join with the appropriate arguments.

More options and information about the kubeadm reset command


# kubectl on a machine external to the cluster #

```
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
# yes, this line shows "xenial" although we're running debian stretch on our desktop machine :
echo "deb http://apt.kubernetes.io/ kubernetes-xenial main" > /etc/apt/sources.list.d/kubernetes.list
apt-get update && apt-get -y install kubectl
```

```
  me@desktop$ mkdir ~/.kube
  me@desktop$ scp pi04:/home/lonelyone/.kube/config $HOME/.kube/

  me@desktop$ get pods --all-namespaces
```


# multi-master setup #

refs:
- https://kubernetes.io/docs/setup/independent/high-availability/
- https://kubernetes.io/docs/setup/independent/ha-topology/



# environments #

- 1 kubernetes namespace per environment (dev, staging, production)
- Role-Based Acces Control --> per users access to namespaces
- network policies : restrict applications to given networks

# monitoring #

- prometheus

